{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Linear Regression with multiple variables: Step by Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "      <th>number of bedrooms</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2104.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>399900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>329900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>369000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1416.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>232000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>539900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>299900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1534.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>314900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1427.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>198999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1380.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>212000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1494.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>242500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1940.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>239999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>347000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1890.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>329999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>4478.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>699900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1268.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>259900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2300.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>449900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1320.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>299900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1236.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>199900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2609.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>499998.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>3031.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>599000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1767.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>252900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1888.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>255000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1604.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>242900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1962.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>259900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>3890.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>573900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>249900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1458.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>464500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>2526.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>469000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>475000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>2637.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>299900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1839.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>349900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>169900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>2040.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>314900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>3137.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>579900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1811.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>285900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1437.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>249900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1239.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>229900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>2132.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>345000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>4215.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>549000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>2162.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>287000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1664.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>368500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>2238.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>329900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>2567.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>314000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>299000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>852.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>179900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1852.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>299900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>239500.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      size  number of bedrooms     price\n",
       "0   2104.0                 3.0  399900.0\n",
       "1   1600.0                 3.0  329900.0\n",
       "2   2400.0                 3.0  369000.0\n",
       "3   1416.0                 2.0  232000.0\n",
       "4   3000.0                 4.0  539900.0\n",
       "5   1985.0                 4.0  299900.0\n",
       "6   1534.0                 3.0  314900.0\n",
       "7   1427.0                 3.0  198999.0\n",
       "8   1380.0                 3.0  212000.0\n",
       "9   1494.0                 3.0  242500.0\n",
       "10  1940.0                 4.0  239999.0\n",
       "11  2000.0                 3.0  347000.0\n",
       "12  1890.0                 3.0  329999.0\n",
       "13  4478.0                 5.0  699900.0\n",
       "14  1268.0                 3.0  259900.0\n",
       "15  2300.0                 4.0  449900.0\n",
       "16  1320.0                 2.0  299900.0\n",
       "17  1236.0                 3.0  199900.0\n",
       "18  2609.0                 4.0  499998.0\n",
       "19  3031.0                 4.0  599000.0\n",
       "20  1767.0                 3.0  252900.0\n",
       "21  1888.0                 2.0  255000.0\n",
       "22  1604.0                 3.0  242900.0\n",
       "23  1962.0                 4.0  259900.0\n",
       "24  3890.0                 3.0  573900.0\n",
       "25  1100.0                 3.0  249900.0\n",
       "26  1458.0                 3.0  464500.0\n",
       "27  2526.0                 3.0  469000.0\n",
       "28  2200.0                 3.0  475000.0\n",
       "29  2637.0                 3.0  299900.0\n",
       "30  1839.0                 2.0  349900.0\n",
       "31  1000.0                 1.0  169900.0\n",
       "32  2040.0                 4.0  314900.0\n",
       "33  3137.0                 3.0  579900.0\n",
       "34  1811.0                 4.0  285900.0\n",
       "35  1437.0                 3.0  249900.0\n",
       "36  1239.0                 3.0  229900.0\n",
       "37  2132.0                 4.0  345000.0\n",
       "38  4215.0                 4.0  549000.0\n",
       "39  2162.0                 4.0  287000.0\n",
       "40  1664.0                 2.0  368500.0\n",
       "41  2238.0                 3.0  329900.0\n",
       "42  2567.0                 4.0  314000.0\n",
       "43  1200.0                 3.0  299000.0\n",
       "44   852.0                 2.0  179900.0\n",
       "45  1852.0                 4.0  299900.0\n",
       "46  1203.0                 3.0  239500.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_table('ex1data2.txt', sep=',', header=None, names=['size', 'number of bedrooms', 'price'], dtype=np.float32)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.loc[:,['size','number of bedrooms']].values\n",
    "Y = data.loc[:,['price']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.mean(X, axis=0)\n",
    "sigma = np.std(X, axis=0)\n",
    "\n",
    "X_train = (X-mu)/sigma\n",
    "Y_train = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression():\n",
    "    \n",
    "    def __call__(self, inputs):\n",
    "        m = inputs.shape[0]\n",
    "        X_ = np.append(np.ones((m,1)), inputs, axis=1)\n",
    "        H = np.dot(X_, self.theta)\n",
    "        return H\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        H = self(inputs)\n",
    "        return H\n",
    "       \n",
    "    def costFunc(self, X, Y):\n",
    "        m = X.shape[0]\n",
    "        H = self(X)\n",
    "        J = (1/(2*m))*np.sum((H - Y)*(H - Y))\n",
    "        return J\n",
    "    \n",
    "    def gradFunc(self, X, Y):\n",
    "        m = X.shape[0]\n",
    "        X_ = np.append(np.ones((m,1)), X, axis=1)\n",
    "        H = self(X)\n",
    "        grad = (1/m)*np.dot(X_.T,H - Y)\n",
    "        return grad\n",
    "                      \n",
    "    def fit(self, X_train, Y_train, epochs=400, learning_rate=0.1):\n",
    "        self.theta = np.random.normal(loc=0.0, scale=0.05, size=(X_train.shape[1]+1, Y_train.shape[1]))\n",
    "        print('Training...')\n",
    "        for epoch in range(epochs):\n",
    "            start_time = time.time()\n",
    "            grad = self.gradFunc(X_train, Y_train)\n",
    "            self.theta = self.theta - learning_rate*grad\n",
    "            loss = self.costFunc(X_train, Y_train)\n",
    "            now = time.time()\n",
    "            duration = now - start_time\n",
    "            print('Epochs {}/{} - Loss: {}'.format(epoch+1, epochs, loss))\n",
    "            print('----- {}s -----'.format(np.round(1000*duration)/1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epochs 1/400 - Loss: 53268140314.21582\n",
      "----- 0.0s -----\n",
      "Epochs 2/400 - Loss: 43388998806.73816\n",
      "----- 0.0s -----\n",
      "Epochs 3/400 - Loss: 35457369228.38772\n",
      "----- 0.0s -----\n",
      "Epochs 4/400 - Loss: 29080510906.537365\n",
      "----- 0.0s -----\n",
      "Epochs 5/400 - Loss: 23947047668.190033\n",
      "----- 0.0s -----\n",
      "Epochs 6/400 - Loss: 19809550601.434906\n",
      "----- 0.0s -----\n",
      "Epochs 7/400 - Loss: 16470994878.965338\n",
      "----- 0.0s -----\n",
      "Epochs 8/400 - Loss: 13774191972.246231\n",
      "----- 0.0s -----\n",
      "Epochs 9/400 - Loss: 11593516117.169756\n",
      "----- 0.0s -----\n",
      "Epochs 10/400 - Loss: 9828406634.161062\n",
      "----- 0.0s -----\n",
      "Epochs 11/400 - Loss: 8398249939.493309\n",
      "----- 0.0s -----\n",
      "Epochs 12/400 - Loss: 7238337264.5793705\n",
      "----- 0.0s -----\n",
      "Epochs 13/400 - Loss: 6296663901.041657\n",
      "----- 0.0s -----\n",
      "Epochs 14/400 - Loss: 5531388862.885388\n",
      "----- 0.0s -----\n",
      "Epochs 15/400 - Loss: 4908814378.227572\n",
      "----- 0.0s -----\n",
      "Epochs 16/400 - Loss: 4401775686.837344\n",
      "----- 0.0s -----\n",
      "Epochs 17/400 - Loss: 3988355528.9577394\n",
      "----- 0.0s -----\n",
      "Epochs 18/400 - Loss: 3650856185.2035666\n",
      "----- 0.0s -----\n",
      "Epochs 19/400 - Loss: 3374976256.0673647\n",
      "----- 0.0s -----\n",
      "Epochs 20/400 - Loss: 3149150523.211787\n",
      "----- 0.0s -----\n",
      "Epochs 21/400 - Loss: 2964019946.787352\n",
      "----- 0.0s -----\n",
      "Epochs 22/400 - Loss: 2812005680.1614127\n",
      "----- 0.0s -----\n",
      "Epochs 23/400 - Loss: 2686966349.892807\n",
      "----- 0.0s -----\n",
      "Epochs 24/400 - Loss: 2583922079.1398854\n",
      "----- 0.0s -----\n",
      "Epochs 25/400 - Loss: 2498832076.3334055\n",
      "----- 0.0s -----\n",
      "Epochs 26/400 - Loss: 2428415260.2362347\n",
      "----- 0.0s -----\n",
      "Epochs 27/400 - Loss: 2370005496.385593\n",
      "----- 0.0s -----\n",
      "Epochs 28/400 - Loss: 2321434694.1148\n",
      "----- 0.0s -----\n",
      "Epochs 29/400 - Loss: 2280938348.14649\n",
      "----- 0.0s -----\n",
      "Epochs 30/400 - Loss: 2247079174.7698646\n",
      "----- 0.0s -----\n",
      "Epochs 31/400 - Loss: 2218685345.3041286\n",
      "----- 0.0s -----\n",
      "Epochs 32/400 - Loss: 2194800502.557353\n",
      "----- 0.0s -----\n",
      "Epochs 33/400 - Loss: 2174643293.7730684\n",
      "----- 0.0s -----\n",
      "Epochs 34/400 - Loss: 2157574593.3876204\n",
      "----- 0.0s -----\n",
      "Epochs 35/400 - Loss: 2143070942.4348497\n",
      "----- 0.0s -----\n",
      "Epochs 36/400 - Loss: 2130703015.8310728\n",
      "----- 0.0s -----\n",
      "Epochs 37/400 - Loss: 2120118157.7540076\n",
      "----- 0.0s -----\n",
      "Epochs 38/400 - Loss: 2111026209.8277042\n",
      "----- 0.0s -----\n",
      "Epochs 39/400 - Loss: 2103188005.581035\n",
      "----- 0.0s -----\n",
      "Epochs 40/400 - Loss: 2096406024.6551988\n",
      "----- 0.0s -----\n",
      "Epochs 41/400 - Loss: 2090516797.1030965\n",
      "----- 0.0s -----\n",
      "Epochs 42/400 - Loss: 2085384726.3497326\n",
      "----- 0.0s -----\n",
      "Epochs 43/400 - Loss: 2080897062.5825925\n",
      "----- 0.0s -----\n",
      "Epochs 44/400 - Loss: 2076959809.4207277\n",
      "----- 0.0s -----\n",
      "Epochs 45/400 - Loss: 2073494388.0099854\n",
      "----- 0.0s -----\n",
      "Epochs 46/400 - Loss: 2070434916.0934937\n",
      "----- 0.0s -----\n",
      "Epochs 47/400 - Loss: 2067725986.6292446\n",
      "----- 0.0s -----\n",
      "Epochs 48/400 - Loss: 2065320852.394988\n",
      "----- 0.0s -----\n",
      "Epochs 49/400 - Loss: 2063179940.7224402\n",
      "----- 0.0s -----\n",
      "Epochs 50/400 - Loss: 2061269636.8356283\n",
      "----- 0.0s -----\n",
      "Epochs 51/400 - Loss: 2059561285.8760314\n",
      "----- 0.0s -----\n",
      "Epochs 52/400 - Loss: 2058030373.100518\n",
      "----- 0.0s -----\n",
      "Epochs 53/400 - Loss: 2056655849.3573053\n",
      "----- 0.0s -----\n",
      "Epochs 54/400 - Loss: 2055419575.1204264\n",
      "----- 0.0s -----\n",
      "Epochs 55/400 - Loss: 2054305861.3693855\n",
      "----- 0.0s -----\n",
      "Epochs 56/400 - Loss: 2053301089.6601162\n",
      "----- 0.0s -----\n",
      "Epochs 57/400 - Loss: 2052393397.0261662\n",
      "----- 0.0s -----\n",
      "Epochs 58/400 - Loss: 2051572414.0205882\n",
      "----- 0.0s -----\n",
      "Epochs 59/400 - Loss: 2050829046.377405\n",
      "----- 0.0s -----\n",
      "Epochs 60/400 - Loss: 2050155292.5319738\n",
      "----- 0.0s -----\n",
      "Epochs 61/400 - Loss: 2049544090.6694725\n",
      "----- 0.0s -----\n",
      "Epochs 62/400 - Loss: 2048989190.1325436\n",
      "----- 0.0s -----\n",
      "Epochs 63/400 - Loss: 2048485042.9636545\n",
      "----- 0.0s -----\n",
      "Epochs 64/400 - Loss: 2048026712.1258204\n",
      "----- 0.0s -----\n",
      "Epochs 65/400 - Loss: 2047609793.570494\n",
      "----- 0.0s -----\n",
      "Epochs 66/400 - Loss: 2047230349.830411\n",
      "----- 0.0s -----\n",
      "Epochs 67/400 - Loss: 2046884853.2298925\n",
      "----- 0.0s -----\n",
      "Epochs 68/400 - Loss: 2046570137.1433437\n",
      "----- 0.0s -----\n",
      "Epochs 69/400 - Loss: 2046283354.00866\n",
      "----- 0.0s -----\n",
      "Epochs 70/400 - Loss: 2046021939.027701\n",
      "----- 0.0s -----\n",
      "Epochs 71/400 - Loss: 2045783578.6703873\n",
      "----- 0.0s -----\n",
      "Epochs 72/400 - Loss: 2045566183.2498424\n",
      "----- 0.0s -----\n",
      "Epochs 73/400 - Loss: 2045367862.9597197\n",
      "----- 0.0s -----\n",
      "Epochs 74/400 - Loss: 2045186906.866384\n",
      "----- 0.0s -----\n",
      "Epochs 75/400 - Loss: 2045021764.432042\n",
      "----- 0.0s -----\n",
      "Epochs 76/400 - Loss: 2044871029.2136521\n",
      "----- 0.0s -----\n",
      "Epochs 77/400 - Loss: 2044733424.4390917\n",
      "----- 0.0s -----\n",
      "Epochs 78/400 - Loss: 2044607790.2088852\n",
      "----- 0.0s -----\n",
      "Epochs 79/400 - Loss: 2044493072.110627\n",
      "----- 0.0s -----\n",
      "Epochs 80/400 - Loss: 2044388311.0653596\n",
      "----- 0.0s -----\n",
      "Epochs 81/400 - Loss: 2044292634.2520053\n",
      "----- 0.0s -----\n",
      "Epochs 82/400 - Loss: 2044205246.9782817\n",
      "----- 0.0s -----\n",
      "Epochs 83/400 - Loss: 2044125425.3852005\n",
      "----- 0.0s -----\n",
      "Epochs 84/400 - Loss: 2044052509.8879828\n",
      "----- 0.0s -----\n",
      "Epochs 85/400 - Loss: 2043985899.2693734\n",
      "----- 0.0s -----\n",
      "Epochs 86/400 - Loss: 2043925045.3525257\n",
      "----- 0.0s -----\n",
      "Epochs 87/400 - Loss: 2043869448.190035\n",
      "----- 0.0s -----\n",
      "Epochs 88/400 - Loss: 2043818651.7137525\n",
      "----- 0.0s -----\n",
      "Epochs 89/400 - Loss: 2043772239.7968364\n",
      "----- 0.0s -----\n",
      "Epochs 90/400 - Loss: 2043729832.6853943\n",
      "----- 0.0s -----\n",
      "Epochs 91/400 - Loss: 2043691083.7620547\n",
      "----- 0.0s -----\n",
      "Epochs 92/400 - Loss: 2043655676.608209\n",
      "----- 0.0s -----\n",
      "Epochs 93/400 - Loss: 2043623322.3353875\n",
      "----- 0.0s -----\n",
      "Epochs 94/400 - Loss: 2043593757.1595078\n",
      "----- 0.0s -----\n",
      "Epochs 95/400 - Loss: 2043566740.1946\n",
      "----- 0.0s -----\n",
      "Epochs 96/400 - Loss: 2043542051.445064\n",
      "----- 0.0s -----\n",
      "Epochs 97/400 - Loss: 2043519489.9777594\n",
      "----- 0.0s -----\n",
      "Epochs 98/400 - Loss: 2043498872.2570894\n",
      "----- 0.0s -----\n",
      "Epochs 99/400 - Loss: 2043480030.6279836\n",
      "----- 0.0s -----\n",
      "Epochs 100/400 - Loss: 2043462811.9331856\n",
      "----- 0.0s -----\n",
      "Epochs 101/400 - Loss: 2043447076.252553\n",
      "----- 0.0s -----\n",
      "Epochs 102/400 - Loss: 2043432695.75332\n",
      "----- 0.0s -----\n",
      "Epochs 103/400 - Loss: 2043419553.6412992\n",
      "----- 0.0s -----\n",
      "Epochs 104/400 - Loss: 2043407543.2039473\n",
      "----- 0.0s -----\n",
      "Epochs 105/400 - Loss: 2043396566.9371014\n",
      "----- 0.0s -----\n",
      "Epochs 106/400 - Loss: 2043386535.7479248\n",
      "----- 0.0s -----\n",
      "Epochs 107/400 - Loss: 2043377368.2273147\n",
      "----- 0.0s -----\n",
      "Epochs 108/400 - Loss: 2043368989.9856122\n",
      "----- 0.0s -----\n",
      "Epochs 109/400 - Loss: 2043361333.0460556\n",
      "----- 0.0s -----\n",
      "Epochs 110/400 - Loss: 2043354335.2908936\n",
      "----- 0.0s -----\n",
      "Epochs 111/400 - Loss: 2043347939.9555314\n",
      "----- 0.0s -----\n",
      "Epochs 112/400 - Loss: 2043342095.1665165\n",
      "----- 0.0s -----\n",
      "Epochs 113/400 - Loss: 2043336753.5195339\n",
      "----- 0.0s -----\n",
      "Epochs 114/400 - Loss: 2043331871.6939118\n",
      "----- 0.0s -----\n",
      "Epochs 115/400 - Loss: 2043327410.10047\n",
      "----- 0.0s -----\n",
      "Epochs 116/400 - Loss: 2043323332.559804\n",
      "----- 0.0s -----\n",
      "Epochs 117/400 - Loss: 2043319606.0083637\n",
      "----- 0.0s -----\n",
      "Epochs 118/400 - Loss: 2043316200.22991\n",
      "----- 0.0s -----\n",
      "Epochs 119/400 - Loss: 2043313087.6101635\n",
      "----- 0.0s -----\n",
      "Epochs 120/400 - Loss: 2043310242.9126089\n",
      "----- 0.0s -----\n",
      "Epochs 121/400 - Loss: 2043307643.0736587\n",
      "----- 0.0s -----\n",
      "Epochs 122/400 - Loss: 2043305267.0154662\n",
      "----- 0.0s -----\n",
      "Epochs 123/400 - Loss: 2043303095.4748983\n",
      "----- 0.0s -----\n",
      "Epochs 124/400 - Loss: 2043301110.8472311\n",
      "----- 0.0s -----\n",
      "Epochs 125/400 - Loss: 2043299297.0433357\n",
      "----- 0.0s -----\n",
      "Epochs 126/400 - Loss: 2043297639.359157\n",
      "----- 0.0s -----\n",
      "Epochs 127/400 - Loss: 2043296124.3564565\n",
      "----- 0.0s -----\n",
      "Epochs 128/400 - Loss: 2043294739.7538097\n",
      "----- 0.0s -----\n",
      "Epochs 129/400 - Loss: 2043293474.3270106\n",
      "----- 0.0s -----\n",
      "Epochs 130/400 - Loss: 2043292317.8180425\n",
      "----- 0.0s -----\n",
      "Epochs 131/400 - Loss: 2043291260.8518994\n",
      "----- 0.0s -----\n",
      "Epochs 132/400 - Loss: 2043290294.8605633\n",
      "----- 0.0s -----\n",
      "Epochs 133/400 - Loss: 2043289412.0135329\n",
      "----- 0.0s -----\n",
      "Epochs 134/400 - Loss: 2043288605.154332\n",
      "----- 0.0s -----\n",
      "Epochs 135/400 - Loss: 2043287867.7424874\n",
      "----- 0.0s -----\n",
      "Epochs 136/400 - Loss: 2043287193.8005006\n",
      "----- 0.0s -----\n",
      "Epochs 137/400 - Loss: 2043286577.8653853\n",
      "----- 0.0s -----\n",
      "Epochs 138/400 - Loss: 2043286014.9443784\n",
      "----- 0.0s -----\n",
      "Epochs 139/400 - Loss: 2043285500.4744642\n",
      "----- 0.0s -----\n",
      "Epochs 140/400 - Loss: 2043285030.2853806\n",
      "----- 0.0s -----\n",
      "Epochs 141/400 - Loss: 2043284600.565814\n",
      "----- 0.0s -----\n",
      "Epochs 142/400 - Loss: 2043284207.832501\n",
      "----- 0.0s -----\n",
      "Epochs 143/400 - Loss: 2043283848.9019911\n",
      "----- 0.0s -----\n",
      "Epochs 144/400 - Loss: 2043283520.8648415\n",
      "----- 0.0s -----\n",
      "Epochs 145/400 - Loss: 2043283221.0620291\n",
      "----- 0.0s -----\n",
      "Epochs 146/400 - Loss: 2043282947.0634015\n",
      "----- 0.0s -----\n",
      "Epochs 147/400 - Loss: 2043282696.6479702\n",
      "----- 0.0s -----\n",
      "Epochs 148/400 - Loss: 2043282467.7859106\n",
      "----- 0.0s -----\n",
      "Epochs 149/400 - Loss: 2043282258.6221092\n",
      "----- 0.0s -----\n",
      "Epochs 150/400 - Loss: 2043282067.4611254\n",
      "----- 0.0s -----\n",
      "Epochs 151/400 - Loss: 2043281892.7534442\n",
      "----- 0.0s -----\n",
      "Epochs 152/400 - Loss: 2043281733.0829203\n",
      "----- 0.0s -----\n",
      "Epochs 153/400 - Loss: 2043281587.155299\n",
      "----- 0.0s -----\n",
      "Epochs 154/400 - Loss: 2043281453.787721\n",
      "----- 0.0s -----\n",
      "Epochs 155/400 - Loss: 2043281331.8991365\n",
      "----- 0.0s -----\n",
      "Epochs 156/400 - Loss: 2043281220.501545\n",
      "----- 0.0s -----\n",
      "Epochs 157/400 - Loss: 2043281118.6919801\n",
      "----- 0.0s -----\n",
      "Epochs 158/400 - Loss: 2043281025.6451983\n",
      "----- 0.0s -----\n",
      "Epochs 159/400 - Loss: 2043280940.6069822\n",
      "----- 0.0s -----\n",
      "Epochs 160/400 - Loss: 2043280862.8880322\n",
      "----- 0.0s -----\n",
      "Epochs 161/400 - Loss: 2043280791.8583753\n",
      "----- 0.0s -----\n",
      "Epochs 162/400 - Loss: 2043280726.9422636\n",
      "----- 0.0s -----\n",
      "Epochs 163/400 - Loss: 2043280667.6135\n",
      "----- 0.0s -----\n",
      "Epochs 164/400 - Loss: 2043280613.391181\n",
      "----- 0.0s -----\n",
      "Epochs 165/400 - Loss: 2043280563.8357933\n",
      "----- 0.0s -----\n",
      "Epochs 166/400 - Loss: 2043280518.545652\n",
      "----- 0.0s -----\n",
      "Epochs 167/400 - Loss: 2043280477.1536455\n",
      "----- 0.0s -----\n",
      "Epochs 168/400 - Loss: 2043280439.3242605\n",
      "----- 0.0s -----\n",
      "Epochs 169/400 - Loss: 2043280404.750862\n",
      "----- 0.0s -----\n",
      "Epochs 170/400 - Loss: 2043280373.153205\n",
      "----- 0.0s -----\n",
      "Epochs 171/400 - Loss: 2043280344.275168\n",
      "----- 0.0s -----\n",
      "Epochs 172/400 - Loss: 2043280317.8826706\n",
      "----- 0.0s -----\n",
      "Epochs 173/400 - Loss: 2043280293.7617834\n",
      "----- 0.0s -----\n",
      "Epochs 174/400 - Loss: 2043280271.7169867\n",
      "----- 0.0s -----\n",
      "Epochs 175/400 - Loss: 2043280251.5695927\n",
      "----- 0.0s -----\n",
      "Epochs 176/400 - Loss: 2043280233.1562889\n",
      "----- 0.0s -----\n",
      "Epochs 177/400 - Loss: 2043280216.3278253\n",
      "----- 0.0s -----\n",
      "Epochs 178/400 - Loss: 2043280200.947791\n",
      "----- 0.0s -----\n",
      "Epochs 179/400 - Loss: 2043280186.8915207\n",
      "----- 0.0s -----\n",
      "Epochs 180/400 - Loss: 2043280174.045077\n",
      "----- 0.0s -----\n",
      "Epochs 181/400 - Loss: 2043280162.3043294\n",
      "----- 0.0s -----\n",
      "Epochs 182/400 - Loss: 2043280151.5741127\n",
      "----- 0.0s -----\n",
      "Epochs 183/400 - Loss: 2043280141.7674475\n",
      "----- 0.0s -----\n",
      "Epochs 184/400 - Loss: 2043280132.8048449\n",
      "----- 0.0s -----\n",
      "Epochs 185/400 - Loss: 2043280124.6136563\n",
      "----- 0.0s -----\n",
      "Epochs 186/400 - Loss: 2043280117.1274853\n",
      "----- 0.0s -----\n",
      "Epochs 187/400 - Loss: 2043280110.2856512\n",
      "----- 0.0s -----\n",
      "Epochs 188/400 - Loss: 2043280104.0326946\n",
      "----- 0.0s -----\n",
      "Epochs 189/400 - Loss: 2043280098.317933\n",
      "----- 0.0s -----\n",
      "Epochs 190/400 - Loss: 2043280093.0950415\n",
      "----- 0.0s -----\n",
      "Epochs 191/400 - Loss: 2043280088.3216858\n",
      "----- 0.0s -----\n",
      "Epochs 192/400 - Loss: 2043280083.9591749\n",
      "----- 0.0s -----\n",
      "Epochs 193/400 - Loss: 2043280079.9721456\n",
      "----- 0.0s -----\n",
      "Epochs 194/400 - Loss: 2043280076.3282812\n",
      "----- 0.0s -----\n",
      "Epochs 195/400 - Loss: 2043280072.9980454\n",
      "----- 0.0s -----\n",
      "Epochs 196/400 - Loss: 2043280069.9544435\n",
      "----- 0.0s -----\n",
      "Epochs 197/400 - Loss: 2043280067.172805\n",
      "----- 0.0s -----\n",
      "Epochs 198/400 - Loss: 2043280064.6305842\n",
      "----- 0.0s -----\n",
      "Epochs 199/400 - Loss: 2043280062.307172\n",
      "----- 0.0s -----\n",
      "Epochs 200/400 - Loss: 2043280060.1837373\n",
      "----- 0.0s -----\n",
      "Epochs 201/400 - Loss: 2043280058.2430663\n",
      "----- 0.0s -----\n",
      "Epochs 202/400 - Loss: 2043280056.4694295\n",
      "----- 0.0s -----\n",
      "Epochs 203/400 - Loss: 2043280054.8484504\n",
      "----- 0.0s -----\n",
      "Epochs 204/400 - Loss: 2043280053.3669884\n",
      "----- 0.0s -----\n",
      "Epochs 205/400 - Loss: 2043280052.0130374\n",
      "----- 0.0s -----\n",
      "Epochs 206/400 - Loss: 2043280050.7756207\n",
      "----- 0.0s -----\n",
      "Epochs 207/400 - Loss: 2043280049.644708\n",
      "----- 0.0s -----\n",
      "Epochs 208/400 - Loss: 2043280048.6111355\n",
      "----- 0.0s -----\n",
      "Epochs 209/400 - Loss: 2043280047.666521\n",
      "----- 0.0s -----\n",
      "Epochs 210/400 - Loss: 2043280046.8032105\n",
      "----- 0.0s -----\n",
      "Epochs 211/400 - Loss: 2043280046.0142055\n",
      "----- 0.0s -----\n",
      "Epochs 212/400 - Loss: 2043280045.2931106\n",
      "----- 0.0s -----\n",
      "Epochs 213/400 - Loss: 2043280044.6340797\n",
      "----- 0.0s -----\n",
      "Epochs 214/400 - Loss: 2043280044.0317714\n",
      "----- 0.0s -----\n",
      "Epochs 215/400 - Loss: 2043280043.4813044\n",
      "----- 0.0s -----\n",
      "Epochs 216/400 - Loss: 2043280042.9782166\n",
      "----- 0.0s -----\n",
      "Epochs 217/400 - Loss: 2043280042.5184307\n",
      "----- 0.0s -----\n",
      "Epochs 218/400 - Loss: 2043280042.0982168\n",
      "----- 0.0s -----\n",
      "Epochs 219/400 - Loss: 2043280041.7141716\n",
      "----- 0.0s -----\n",
      "Epochs 220/400 - Loss: 2043280041.3631809\n",
      "----- 0.0s -----\n",
      "Epochs 221/400 - Loss: 2043280041.0424008\n",
      "----- 0.0s -----\n",
      "Epochs 222/400 - Loss: 2043280040.74923\n",
      "----- 0.0s -----\n",
      "Epochs 223/400 - Loss: 2043280040.4812927\n",
      "----- 0.0s -----\n",
      "Epochs 224/400 - Loss: 2043280040.2364163\n",
      "----- 0.0s -----\n",
      "Epochs 225/400 - Loss: 2043280040.0126164\n",
      "----- 0.0s -----\n",
      "Epochs 226/400 - Loss: 2043280039.808079\n",
      "----- 0.0s -----\n",
      "Epochs 227/400 - Loss: 2043280039.6211462\n",
      "----- 0.0s -----\n",
      "Epochs 228/400 - Loss: 2043280039.450304\n",
      "----- 0.0s -----\n",
      "Epochs 229/400 - Loss: 2043280039.2941651\n",
      "----- 0.0s -----\n",
      "Epochs 230/400 - Loss: 2043280039.1514654\n",
      "----- 0.0s -----\n",
      "Epochs 231/400 - Loss: 2043280039.0210476\n",
      "----- 0.0s -----\n",
      "Epochs 232/400 - Loss: 2043280038.9018555\n",
      "----- 0.0s -----\n",
      "Epochs 233/400 - Loss: 2043280038.7929215\n",
      "----- 0.0s -----\n",
      "Epochs 234/400 - Loss: 2043280038.693364\n",
      "----- 0.0s -----\n",
      "Epochs 235/400 - Loss: 2043280038.602375\n",
      "----- 0.0s -----\n",
      "Epochs 236/400 - Loss: 2043280038.5192184\n",
      "----- 0.0s -----\n",
      "Epochs 237/400 - Loss: 2043280038.4432178\n",
      "----- 0.0s -----\n",
      "Epochs 238/400 - Loss: 2043280038.3737595\n",
      "----- 0.0s -----\n",
      "Epochs 239/400 - Loss: 2043280038.3102798\n",
      "----- 0.0s -----\n",
      "Epochs 240/400 - Loss: 2043280038.252263\n",
      "----- 0.0s -----\n",
      "Epochs 241/400 - Loss: 2043280038.1992404\n",
      "----- 0.0s -----\n",
      "Epochs 242/400 - Loss: 2043280038.150781\n",
      "----- 0.0s -----\n",
      "Epochs 243/400 - Loss: 2043280038.1064918\n",
      "----- 0.0s -----\n",
      "Epochs 244/400 - Loss: 2043280038.066016\n",
      "----- 0.0s -----\n",
      "Epochs 245/400 - Loss: 2043280038.029023\n",
      "----- 0.0s -----\n",
      "Epochs 246/400 - Loss: 2043280037.9952145\n",
      "----- 0.0s -----\n",
      "Epochs 247/400 - Loss: 2043280037.9643161\n",
      "----- 0.0s -----\n",
      "Epochs 248/400 - Loss: 2043280037.9360764\n",
      "----- 0.0s -----\n",
      "Epochs 249/400 - Loss: 2043280037.9102678\n",
      "----- 0.0s -----\n",
      "Epochs 250/400 - Loss: 2043280037.8866804\n",
      "----- 0.0s -----\n",
      "Epochs 251/400 - Loss: 2043280037.8651235\n",
      "----- 0.0s -----\n",
      "Epochs 252/400 - Loss: 2043280037.845422\n",
      "----- 0.0s -----\n",
      "Epochs 253/400 - Loss: 2043280037.827416\n",
      "----- 0.0s -----\n",
      "Epochs 254/400 - Loss: 2043280037.8109598\n",
      "----- 0.0s -----\n",
      "Epochs 255/400 - Loss: 2043280037.7959204\n",
      "----- 0.0s -----\n",
      "Epochs 256/400 - Loss: 2043280037.7821746\n",
      "----- 0.0s -----\n",
      "Epochs 257/400 - Loss: 2043280037.7696116\n",
      "----- 0.0s -----\n",
      "Epochs 258/400 - Loss: 2043280037.7581306\n",
      "----- 0.0s -----\n",
      "Epochs 259/400 - Loss: 2043280037.7476375\n",
      "----- 0.0s -----\n",
      "Epochs 260/400 - Loss: 2043280037.738048\n",
      "----- 0.0s -----\n",
      "Epochs 261/400 - Loss: 2043280037.729284\n",
      "----- 0.0s -----\n",
      "Epochs 262/400 - Loss: 2043280037.7212732\n",
      "----- 0.0s -----\n",
      "Epochs 263/400 - Loss: 2043280037.7139535\n",
      "----- 0.0s -----\n",
      "Epochs 264/400 - Loss: 2043280037.707262\n",
      "----- 0.0s -----\n",
      "Epochs 265/400 - Loss: 2043280037.7011478\n",
      "----- 0.0s -----\n",
      "Epochs 266/400 - Loss: 2043280037.6955595\n",
      "----- 0.0s -----\n",
      "Epochs 267/400 - Loss: 2043280037.6904526\n",
      "----- 0.0s -----\n",
      "Epochs 268/400 - Loss: 2043280037.6857848\n",
      "----- 0.0s -----\n",
      "Epochs 269/400 - Loss: 2043280037.681518\n",
      "----- 0.0s -----\n",
      "Epochs 270/400 - Loss: 2043280037.6776195\n",
      "----- 0.0s -----\n",
      "Epochs 271/400 - Loss: 2043280037.6740563\n",
      "----- 0.0s -----\n",
      "Epochs 272/400 - Loss: 2043280037.6707997\n",
      "----- 0.0s -----\n",
      "Epochs 273/400 - Loss: 2043280037.6678236\n",
      "----- 0.0s -----\n",
      "Epochs 274/400 - Loss: 2043280037.6651037\n",
      "----- 0.0s -----\n",
      "Epochs 275/400 - Loss: 2043280037.6626174\n",
      "----- 0.0s -----\n",
      "Epochs 276/400 - Loss: 2043280037.660345\n",
      "----- 0.0s -----\n",
      "Epochs 277/400 - Loss: 2043280037.6582694\n",
      "----- 0.0s -----\n",
      "Epochs 278/400 - Loss: 2043280037.656371\n",
      "----- 0.0s -----\n",
      "Epochs 279/400 - Loss: 2043280037.6546366\n",
      "----- 0.0s -----\n",
      "Epochs 280/400 - Loss: 2043280037.6530514\n",
      "----- 0.0s -----\n",
      "Epochs 281/400 - Loss: 2043280037.6516027\n",
      "----- 0.0s -----\n",
      "Epochs 282/400 - Loss: 2043280037.6502786\n",
      "----- 0.0s -----\n",
      "Epochs 283/400 - Loss: 2043280037.6490684\n",
      "----- 0.0s -----\n",
      "Epochs 284/400 - Loss: 2043280037.6479628\n",
      "----- 0.0s -----\n",
      "Epochs 285/400 - Loss: 2043280037.6469529\n",
      "----- 0.0s -----\n",
      "Epochs 286/400 - Loss: 2043280037.6460292\n",
      "----- 0.0s -----\n",
      "Epochs 287/400 - Loss: 2043280037.6451838\n",
      "----- 0.0s -----\n",
      "Epochs 288/400 - Loss: 2043280037.644413\n",
      "----- 0.0s -----\n",
      "Epochs 289/400 - Loss: 2043280037.6437073\n",
      "----- 0.0s -----\n",
      "Epochs 290/400 - Loss: 2043280037.6430628\n",
      "----- 0.0s -----\n",
      "Epochs 291/400 - Loss: 2043280037.642474\n",
      "----- 0.0s -----\n",
      "Epochs 292/400 - Loss: 2043280037.6419353\n",
      "----- 0.0s -----\n",
      "Epochs 293/400 - Loss: 2043280037.641444\n",
      "----- 0.0s -----\n",
      "Epochs 294/400 - Loss: 2043280037.6409948\n",
      "----- 0.0s -----\n",
      "Epochs 295/400 - Loss: 2043280037.640583\n",
      "----- 0.0s -----\n",
      "Epochs 296/400 - Loss: 2043280037.640208\n",
      "----- 0.0s -----\n",
      "Epochs 297/400 - Loss: 2043280037.6398652\n",
      "----- 0.0s -----\n",
      "Epochs 298/400 - Loss: 2043280037.639551\n",
      "----- 0.0s -----\n",
      "Epochs 299/400 - Loss: 2043280037.6392646\n",
      "----- 0.0s -----\n",
      "Epochs 300/400 - Loss: 2043280037.6390023\n",
      "----- 0.0s -----\n",
      "Epochs 301/400 - Loss: 2043280037.6387634\n",
      "----- 0.0s -----\n",
      "Epochs 302/400 - Loss: 2043280037.6385438\n",
      "----- 0.0s -----\n",
      "Epochs 303/400 - Loss: 2043280037.6383438\n",
      "----- 0.0s -----\n",
      "Epochs 304/400 - Loss: 2043280037.6381607\n",
      "----- 0.0s -----\n",
      "Epochs 305/400 - Loss: 2043280037.6379943\n",
      "----- 0.0s -----\n",
      "Epochs 306/400 - Loss: 2043280037.6378415\n",
      "----- 0.0s -----\n",
      "Epochs 307/400 - Loss: 2043280037.6377015\n",
      "----- 0.0s -----\n",
      "Epochs 308/400 - Loss: 2043280037.6375742\n",
      "----- 0.0s -----\n",
      "Epochs 309/400 - Loss: 2043280037.6374583\n",
      "----- 0.0s -----\n",
      "Epochs 310/400 - Loss: 2043280037.637351\n",
      "----- 0.0s -----\n",
      "Epochs 311/400 - Loss: 2043280037.637254\n",
      "----- 0.0s -----\n",
      "Epochs 312/400 - Loss: 2043280037.6371648\n",
      "----- 0.0s -----\n",
      "Epochs 313/400 - Loss: 2043280037.6370835\n",
      "----- 0.0s -----\n",
      "Epochs 314/400 - Loss: 2043280037.6370099\n",
      "----- 0.0s -----\n",
      "Epochs 315/400 - Loss: 2043280037.6369417\n",
      "----- 0.0s -----\n",
      "Epochs 316/400 - Loss: 2043280037.6368794\n",
      "----- 0.0s -----\n",
      "Epochs 317/400 - Loss: 2043280037.6368222\n",
      "----- 0.0s -----\n",
      "Epochs 318/400 - Loss: 2043280037.636771\n",
      "----- 0.0s -----\n",
      "Epochs 319/400 - Loss: 2043280037.6367235\n",
      "----- 0.0s -----\n",
      "Epochs 320/400 - Loss: 2043280037.6366806\n",
      "----- 0.0s -----\n",
      "Epochs 321/400 - Loss: 2043280037.6366405\n",
      "----- 0.0s -----\n",
      "Epochs 322/400 - Loss: 2043280037.636604\n",
      "----- 0.0s -----\n",
      "Epochs 323/400 - Loss: 2043280037.636571\n",
      "----- 0.0s -----\n",
      "Epochs 324/400 - Loss: 2043280037.6365414\n",
      "----- 0.0s -----\n",
      "Epochs 325/400 - Loss: 2043280037.6365135\n",
      "----- 0.0s -----\n",
      "Epochs 326/400 - Loss: 2043280037.6364875\n",
      "----- 0.0s -----\n",
      "Epochs 327/400 - Loss: 2043280037.636465\n",
      "----- 0.0s -----\n",
      "Epochs 328/400 - Loss: 2043280037.6364436\n",
      "----- 0.0s -----\n",
      "Epochs 329/400 - Loss: 2043280037.6364253\n",
      "----- 0.0s -----\n",
      "Epochs 330/400 - Loss: 2043280037.6364064\n",
      "----- 0.0s -----\n",
      "Epochs 331/400 - Loss: 2043280037.6363902\n",
      "----- 0.0s -----\n",
      "Epochs 332/400 - Loss: 2043280037.6363766\n",
      "----- 0.0s -----\n",
      "Epochs 333/400 - Loss: 2043280037.6363628\n",
      "----- 0.0s -----\n",
      "Epochs 334/400 - Loss: 2043280037.63635\n",
      "----- 0.0s -----\n",
      "Epochs 335/400 - Loss: 2043280037.6363401\n",
      "----- 0.0s -----\n",
      "Epochs 336/400 - Loss: 2043280037.6363285\n",
      "----- 0.0s -----\n",
      "Epochs 337/400 - Loss: 2043280037.63632\n",
      "----- 0.0s -----\n",
      "Epochs 338/400 - Loss: 2043280037.6363108\n",
      "----- 0.0s -----\n",
      "Epochs 339/400 - Loss: 2043280037.6363041\n",
      "----- 0.0s -----\n",
      "Epochs 340/400 - Loss: 2043280037.636296\n",
      "----- 0.0s -----\n",
      "Epochs 341/400 - Loss: 2043280037.6362898\n",
      "----- 0.0s -----\n",
      "Epochs 342/400 - Loss: 2043280037.6362834\n",
      "----- 0.0s -----\n",
      "Epochs 343/400 - Loss: 2043280037.6362782\n",
      "----- 0.0s -----\n",
      "Epochs 344/400 - Loss: 2043280037.6362734\n",
      "----- 0.0s -----\n",
      "Epochs 345/400 - Loss: 2043280037.6362684\n",
      "----- 0.0s -----\n",
      "Epochs 346/400 - Loss: 2043280037.6362646\n",
      "----- 0.0s -----\n",
      "Epochs 347/400 - Loss: 2043280037.6362605\n",
      "----- 0.0s -----\n",
      "Epochs 348/400 - Loss: 2043280037.6362574\n",
      "----- 0.0s -----\n",
      "Epochs 349/400 - Loss: 2043280037.636254\n",
      "----- 0.0s -----\n",
      "Epochs 350/400 - Loss: 2043280037.636251\n",
      "----- 0.0s -----\n",
      "Epochs 351/400 - Loss: 2043280037.6362484\n",
      "----- 0.0s -----\n",
      "Epochs 352/400 - Loss: 2043280037.636246\n",
      "----- 0.0s -----\n",
      "Epochs 353/400 - Loss: 2043280037.6362438\n",
      "----- 0.0s -----\n",
      "Epochs 354/400 - Loss: 2043280037.6362417\n",
      "----- 0.0s -----\n",
      "Epochs 355/400 - Loss: 2043280037.6362398\n",
      "----- 0.0s -----\n",
      "Epochs 356/400 - Loss: 2043280037.636238\n",
      "----- 0.0s -----\n",
      "Epochs 357/400 - Loss: 2043280037.636237\n",
      "----- 0.0s -----\n",
      "Epochs 358/400 - Loss: 2043280037.6362357\n",
      "----- 0.0s -----\n",
      "Epochs 359/400 - Loss: 2043280037.636234\n",
      "----- 0.0s -----\n",
      "Epochs 360/400 - Loss: 2043280037.6362324\n",
      "----- 0.0s -----\n",
      "Epochs 361/400 - Loss: 2043280037.6362317\n",
      "----- 0.0s -----\n",
      "Epochs 362/400 - Loss: 2043280037.6362307\n",
      "----- 0.0s -----\n",
      "Epochs 363/400 - Loss: 2043280037.63623\n",
      "----- 0.0s -----\n",
      "Epochs 364/400 - Loss: 2043280037.6362286\n",
      "----- 0.0s -----\n",
      "Epochs 365/400 - Loss: 2043280037.6362286\n",
      "----- 0.0s -----\n",
      "Epochs 366/400 - Loss: 2043280037.6362276\n",
      "----- 0.0s -----\n",
      "Epochs 367/400 - Loss: 2043280037.636227\n",
      "----- 0.0s -----\n",
      "Epochs 368/400 - Loss: 2043280037.6362262\n",
      "----- 0.0s -----\n",
      "Epochs 369/400 - Loss: 2043280037.6362255\n",
      "----- 0.0s -----\n",
      "Epochs 370/400 - Loss: 2043280037.6362252\n",
      "----- 0.0s -----\n",
      "Epochs 371/400 - Loss: 2043280037.6362252\n",
      "----- 0.0s -----\n",
      "Epochs 372/400 - Loss: 2043280037.636225\n",
      "----- 0.0s -----\n",
      "Epochs 373/400 - Loss: 2043280037.6362236\n",
      "----- 0.0s -----\n",
      "Epochs 374/400 - Loss: 2043280037.6362236\n",
      "----- 0.0s -----\n",
      "Epochs 375/400 - Loss: 2043280037.6362228\n",
      "----- 0.0s -----\n",
      "Epochs 376/400 - Loss: 2043280037.6362226\n",
      "----- 0.0s -----\n",
      "Epochs 377/400 - Loss: 2043280037.6362224\n",
      "----- 0.0s -----\n",
      "Epochs 378/400 - Loss: 2043280037.6362228\n",
      "----- 0.0s -----\n",
      "Epochs 379/400 - Loss: 2043280037.6362226\n",
      "----- 0.0s -----\n",
      "Epochs 380/400 - Loss: 2043280037.6362216\n",
      "----- 0.0s -----\n",
      "Epochs 381/400 - Loss: 2043280037.6362216\n",
      "----- 0.0s -----\n",
      "Epochs 382/400 - Loss: 2043280037.6362214\n",
      "----- 0.0s -----\n",
      "Epochs 383/400 - Loss: 2043280037.6362214\n",
      "----- 0.0s -----\n",
      "Epochs 384/400 - Loss: 2043280037.6362216\n",
      "----- 0.0s -----\n",
      "Epochs 385/400 - Loss: 2043280037.6362216\n",
      "----- 0.0s -----\n",
      "Epochs 386/400 - Loss: 2043280037.636221\n",
      "----- 0.0s -----\n",
      "Epochs 387/400 - Loss: 2043280037.636221\n",
      "----- 0.0s -----\n",
      "Epochs 388/400 - Loss: 2043280037.6362207\n",
      "----- 0.0s -----\n",
      "Epochs 389/400 - Loss: 2043280037.6362214\n",
      "----- 0.0s -----\n",
      "Epochs 390/400 - Loss: 2043280037.6362207\n",
      "----- 0.0s -----\n",
      "Epochs 391/400 - Loss: 2043280037.636221\n",
      "----- 0.0s -----\n",
      "Epochs 392/400 - Loss: 2043280037.636221\n",
      "----- 0.0s -----\n",
      "Epochs 393/400 - Loss: 2043280037.6362207\n",
      "----- 0.0s -----\n",
      "Epochs 394/400 - Loss: 2043280037.6362214\n",
      "----- 0.0s -----\n",
      "Epochs 395/400 - Loss: 2043280037.63622\n",
      "----- 0.0s -----\n",
      "Epochs 396/400 - Loss: 2043280037.6362205\n",
      "----- 0.0s -----\n",
      "Epochs 397/400 - Loss: 2043280037.6362205\n",
      "----- 0.0s -----\n",
      "Epochs 398/400 - Loss: 2043280037.6362207\n",
      "----- 0.0s -----\n",
      "Epochs 399/400 - Loss: 2043280037.6362207\n",
      "----- 0.0s -----\n",
      "Epochs 400/400 - Loss: 2043280037.6362207\n",
      "----- 0.0s -----\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4086560075.2724414"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*model.costFunc(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[356283.1110963 ],\n",
       "       [286120.93164502],\n",
       "       [397489.4728382 ],\n",
       "       [269244.18380728],\n",
       "       [472277.84976714],\n",
       "       [330979.02120125],\n",
       "       [276933.02565925],\n",
       "       [262037.48753131],\n",
       "       [255494.58053364],\n",
       "       [271364.60055667],\n",
       "       [324714.5405145 ],\n",
       "       [341805.20041959],\n",
       "       [326492.02668592],\n",
       "       [669293.21083291],\n",
       "       [239902.98763698],\n",
       "       [374830.38437761],\n",
       "       [255879.96225637],\n",
       "       [235448.24494548],\n",
       "       [417846.48166841],\n",
       "       [476593.39194265],\n",
       "       [309369.11395043],\n",
       "       [334951.62382885],\n",
       "       [286677.77285056],\n",
       "       [327777.17570718],\n",
       "       [604913.36988346],\n",
       "       [216515.59176839],\n",
       "       [266353.01665963],\n",
       "       [415030.01647784],\n",
       "       [369647.33427811],\n",
       "       [430482.40233492],\n",
       "       [328130.30112111],\n",
       "       [220070.55996721],\n",
       "       [338635.60816365],\n",
       "       [500087.73568795],\n",
       "       [306756.3643398 ],\n",
       "       [263429.59054516],\n",
       "       [235865.87911143],\n",
       "       [351442.99013992],\n",
       "       [641418.81933947],\n",
       "       [355619.31059775],\n",
       "       [303768.43314282],\n",
       "       [374937.34203971],\n",
       "       [411999.63596306],\n",
       "       [230436.66104844],\n",
       "       [190729.36507124],\n",
       "       [312464.00137467],\n",
       "       [230854.28869079]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = model.predict(X_train)\n",
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[399900.],\n",
       "       [329900.],\n",
       "       [369000.],\n",
       "       [232000.],\n",
       "       [539900.],\n",
       "       [299900.],\n",
       "       [314900.],\n",
       "       [198999.],\n",
       "       [212000.],\n",
       "       [242500.],\n",
       "       [239999.],\n",
       "       [347000.],\n",
       "       [329999.],\n",
       "       [699900.],\n",
       "       [259900.],\n",
       "       [449900.],\n",
       "       [299900.],\n",
       "       [199900.],\n",
       "       [499998.],\n",
       "       [599000.],\n",
       "       [252900.],\n",
       "       [255000.],\n",
       "       [242900.],\n",
       "       [259900.],\n",
       "       [573900.],\n",
       "       [249900.],\n",
       "       [464500.],\n",
       "       [469000.],\n",
       "       [475000.],\n",
       "       [299900.],\n",
       "       [349900.],\n",
       "       [169900.],\n",
       "       [314900.],\n",
       "       [579900.],\n",
       "       [285900.],\n",
       "       [249900.],\n",
       "       [229900.],\n",
       "       [345000.],\n",
       "       [549000.],\n",
       "       [287000.],\n",
       "       [368500.],\n",
       "       [329900.],\n",
       "       [314000.],\n",
       "       [299000.],\n",
       "       [179900.],\n",
       "       [299900.],\n",
       "       [239500.]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[293081.46468585]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1650, 3]])\n",
    "x_norm = (x-mu)/sigma\n",
    "model.predict(x_norm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
